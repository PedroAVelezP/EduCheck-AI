{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AsrTY9j5ypp_",
        "wcUKoKzKiDR8"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PedroAVelezP/EduCheck-AI/blob/Deepseek_R1/Notebooks/Debug_EduCheck_Deepseek.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Librerias"
      ],
      "metadata": {
        "id": "AsrTY9j5ypp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/PedroAVelezP/EduCheck-AI.git"
      ],
      "metadata": {
        "id": "-ayFiwjQon1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd EduCheck-AI\n",
        "!git checkout Deepseek_R1"
      ],
      "metadata": {
        "id": "xxwbbiXvB8gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -b Deepseek_R1 --single-branch https://github.com/PedroAVelezP/EduCheck-AI.git"
      ],
      "metadata": {
        "id": "yYl9MMHuB5zP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5l3TLSwxbGc"
      },
      "outputs": [],
      "source": [
        "!pip install gradio\n",
        "!sudo apt-get install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "import numpy as np\n",
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "\n",
        "import time\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "\n",
        "def ollama():\n",
        "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])"
      ],
      "metadata": {
        "id": "E1uO5JClxoFD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ollama_thread = threading.Thread(target=ollama)\n",
        "ollama_thread.start()\n",
        "ollama()\n",
        "!ollama pull deepseek-r1:8b\n",
        "clear_output()\n",
        "\n",
        "!pip install -U lightrag[ollama]\n",
        "\n",
        "from lightrag.core.generator import Generator\n",
        "from lightrag.core.component import Component\n",
        "from lightrag.core.model_client import ModelClient\n",
        "from lightrag.components.model_client import OllamaClient, GroqAPIClient"
      ],
      "metadata": {
        "id": "oeQtbrT0x-t9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejecución"
      ],
      "metadata": {
        "id": "-7VXXLWaoi_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/EduCheck-AI\n",
        "!python main.py --preset default"
      ],
      "metadata": {
        "id": "ghYqnJYqXRJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Debug"
      ],
      "metadata": {
        "id": "XkbZRgxLAobQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ollama_thread = threading.Thread(target=ollama)\n",
        "ollama_thread.start()\n",
        "ollama()\n",
        "!ollama pull deepseek-r1:8b\n",
        "clear_output()\n",
        "\n",
        "!pip install -U lightrag[ollama]\n",
        "\n",
        "from lightrag.core.generator import Generator\n",
        "from lightrag.core.component import Component\n",
        "from lightrag.core.model_client import ModelClient\n",
        "from lightrag.components.model_client import OllamaClient, GroqAPIClient"
      ],
      "metadata": {
        "id": "vhYZdbcZ2Avt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleQA(Component):\n",
        "    def __init__(self, model_client: OllamaClient, model_kwargs: dict):\n",
        "        super().__init__()\n",
        "        self.generator = Generator(\n",
        "            model_client=model_client,\n",
        "            model_kwargs=model_kwargs,\n",
        "            template=qa_template,\n",
        "        )\n",
        "\n",
        "    def call(self, input: dict) -> str:\n",
        "        return self.generator.call({\"input_str\": str(input)})\n",
        "\n",
        "    async def acall(self, input: dict) -> str:\n",
        "        return await self.generator.acall({\"input_str\": str(input)})\n",
        "\n",
        "qa_template = \"\"\"<SYS>\n",
        "You are an AI specialized in scoring the closeness between the correct answer (CA) and the given answer (GA), for this you will have to analyze the question that was being asked before getting the given answer.\n",
        "\n",
        "The output format should be as follows:\n",
        "###Score: (0-100)\n",
        "###Justification:\n",
        "\n",
        "Consider the initial question to give your final score, since being specialized topics it may be the case that the answer given (GA) is completely correct despite being different from the answer marked as correct (CA), at the same time consider the degree of knowledge that the person seems to have of the topic, since it may be the case where the person tries to guess the correct answer.\n",
        "The question asked was: ¿Qué hormona regula los niveles de glucosa en sangre y dónde se produce?\n",
        "The correct answer should be: La insulina, producida por las células beta del páncreas.\n",
        "The answer given was: insulina\n",
        "\n",
        "</SYS>\n",
        "\"\"\"\n",
        "\n",
        "model = {\n",
        "    \"model_client\": OllamaClient(),\n",
        "    \"model_kwargs\": {\"model\": \"deepseek-r1:8b\"}\n",
        "}\n",
        "qa = SimpleQA(**model)\n",
        "Entrada = qa(f\"\")\n",
        "respuesta_generada = Entrada.data\n",
        "\n",
        "print(respuesta_generada)"
      ],
      "metadata": {
        "id": "R39qEB582QH6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}