{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AsrTY9j5ypp_",
        "wcUKoKzKiDR8"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PedroAVelezP/EduCheck-AI/blob/Deepseek_R1/Notebooks/Debug_EduCheck_Deepseek.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Librerias"
      ],
      "metadata": {
        "id": "AsrTY9j5ypp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/PedroAVelezP/EduCheck-AI.git"
      ],
      "metadata": {
        "id": "-ayFiwjQon1Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89466062-2e37-449d-d059-fb3d0dee7bb6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'EduCheck-AI'...\n",
            "remote: Enumerating objects: 118, done.\u001b[K\n",
            "remote: Counting objects: 100% (118/118), done.\u001b[K\n",
            "remote: Compressing objects: 100% (111/111), done.\u001b[K\n",
            "remote: Total 118 (delta 50), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (118/118), 71.39 KiB | 5.95 MiB/s, done.\n",
            "Resolving deltas: 100% (50/50), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5l3TLSwxbGc"
      },
      "outputs": [],
      "source": [
        "!pip install gradio\n",
        "!sudo apt-get install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "import numpy as np\n",
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "\n",
        "import time\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "\n",
        "def ollama():\n",
        "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])"
      ],
      "metadata": {
        "id": "E1uO5JClxoFD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ollama_thread = threading.Thread(target=ollama)\n",
        "ollama_thread.start()\n",
        "ollama()\n",
        "!ollama pull llama3.2\n",
        "clear_output()\n",
        "\n",
        "!pip install -U lightrag[ollama]\n",
        "\n",
        "from lightrag.core.generator import Generator\n",
        "from lightrag.core.component import Component\n",
        "from lightrag.core.model_client import ModelClient\n",
        "from lightrag.components.model_client import OllamaClient, GroqAPIClient"
      ],
      "metadata": {
        "id": "oeQtbrT0x-t9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejecución"
      ],
      "metadata": {
        "id": "-7VXXLWaoi_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/EduCheck-AI\n",
        "!python main.py --preset debug"
      ],
      "metadata": {
        "id": "ghYqnJYqXRJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Debug"
      ],
      "metadata": {
        "id": "XkbZRgxLAobQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ollama_thread = threading.Thread(target=ollama)\n",
        "ollama_thread.start()\n",
        "ollama()\n",
        "!ollama pull deepseek-r1:8b\n",
        "clear_output()\n",
        "\n",
        "!pip install -U lightrag[ollama]\n",
        "\n",
        "from lightrag.core.generator import Generator\n",
        "from lightrag.core.component import Component\n",
        "from lightrag.core.model_client import ModelClient\n",
        "from lightrag.components.model_client import OllamaClient, GroqAPIClient"
      ],
      "metadata": {
        "id": "vhYZdbcZ2Avt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleQA(Component):\n",
        "    def __init__(self, model_client: OllamaClient, model_kwargs: dict):\n",
        "        super().__init__()\n",
        "        self.generator = Generator(\n",
        "            model_client=model_client,\n",
        "            model_kwargs=model_kwargs,\n",
        "            template=qa_template,\n",
        "        )\n",
        "\n",
        "    def call(self, input: dict) -> str:\n",
        "        return self.generator.call({\"input_str\": str(input)})\n",
        "\n",
        "    async def acall(self, input: dict) -> str:\n",
        "        return await self.generator.acall({\"input_str\": str(input)})\n",
        "\n",
        "qa_template = \"\"\"<SYS>\n",
        "You are an AI specialized in scoring the closeness between the correct answer (CA) and the given answer (GA), for this you will have to analyze the question that was being asked before getting the given answer.\n",
        "\n",
        "The output format should be as follows:\n",
        "###Score: (0-100)\n",
        "###Justification:\n",
        "\n",
        "Consider the initial question to give your final score, since being specialized topics it may be the case that the answer given (GA) is completely correct despite being different from the answer marked as correct (CA), at the same time consider the degree of knowledge that the person seems to have of the topic, since it may be the case where the person tries to guess the correct answer.\n",
        "The question asked was: ¿Qué hormona regula los niveles de glucosa en sangre y dónde se produce?\n",
        "The correct answer should be: La insulina, producida por las células beta del páncreas.\n",
        "The answer given was: insulina\n",
        "\n",
        "</SYS>\n",
        "\"\"\"\n",
        "\n",
        "# Crear cliente y llamada a SimpleQA\n",
        "model = {\n",
        "    \"model_client\": OllamaClient(),\n",
        "    \"model_kwargs\": {\"model\": \"deepseek-r1:8b\"}\n",
        "}\n",
        "qa = SimpleQA(**model)\n",
        "Entrada = qa(f\"\")\n",
        "respuesta_generada = Entrada.data\n",
        "\n",
        "print(respuesta_generada)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R39qEB582QH6",
        "outputId": "0b013b55-265e-46f8-9184-76bb67223637"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:lightrag.core.prompt_builder:Key input_str does not exist in the prompt_kwargs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<think>\n",
            "Okay, so I need to figure out how to score the closeness between the correct answer and the given answer for this question. The question is about which hormone regulates blood glucose levels and where it's produced.\n",
            "\n",
            "First, let me look at the correct answer. It says that insulin is produced by beta cells in the pancreas. That makes sense because I remember from biology class that insulin is released by the pancreas to help regulate blood sugar.\n",
            "\n",
            "Now, the given answer is just \"insulina,\" which is Spanish for insulin. So the GA (given answer) correctly identifies the hormone as insulin without specifying its source or function. But the correct answer not only names the hormone but also mentions where it's produced.\n",
            "\n",
            "I should consider how similar these two answers are. The GA is missing some details about the production location, but it does get the name right. Maybe the person knew the answer but didn't include all the required information. Alternatively, they might have just guessed the correct term without knowing where it's made.\n",
            "\n",
            "Since the question asks for both the hormone and its production site, the CA provides a more complete answer. However, the GA is partially correct by identifying the hormone correctly. I wonder if this was a deliberate omission or an oversight. If the person knew insulin was the right answer but forgot to mention the source, that's one thing. But if they didn't know and just guessed, it might be a coincidence.\n",
            "\n",
            "I should also think about possible scores. Since the GA is correct in part, maybe around 70? They got the main point right but missed some details. I don't want to penalize them too harshly for not mentioning the source because it's a detail they might have overlooked.\n",
            "</think>\n",
            "\n",
            "The given answer correctly identifies the hormone as insulin, which is partially accurate. While it misses specifying where it's produced, it accurately names the hormone. \n",
            "\n",
            "###Score: 70\n",
            "###Justification:\n",
            "The answer \"insulina\" correctly identifies the hormone but omits the production site. Although it's a detail, the main term is correct. This shows some knowledge of the topic, possibly through guessing or remembering the name without additional details.\n"
          ]
        }
      ]
    }
  ]
}